# ============================================================
# dos_config.yaml
# Ensemble-TranBiLSTM (Phase 2) -- CIC-IDS2017 DoS subset
# Paper: Wang et al., 2023 -- Proposed improvement
#
# Phase 2 changes over Phase 1:
#   Spatial:      ResNet-18 -> MobileNetV2 + EfficientNet-B0 ensemble
#   Augmentation: SMOTE-ENN -> CTGAN
#   Attention:    softmax O(n^2) -> linear kernel O(n)
#   Recurrence:   BiLSTM (IDENTICAL to Phase 1 -- paper preserved)
#
# Twinned to ExistingImplementation/configs/dos_config.yaml:
#   Same section names, same key names, same hyperparameters
#   (batch_size, lr, dropout, max_epochs, patience) for fair comparison.
# ============================================================

experiment:
  name: "ensemble_tranbilstm_dos"
  description: "CIC-IDS2017 DoS subset -- MobileNetV2+EfficientNetB0 ensemble + CTGAN"
  seed: 42

# ----------------------------------------------------------
# Dataset
# ----------------------------------------------------------
data:
  dataset:        "CIC-IDS2017"
  csv_path:       "data/raw/Wednesday-workingHours.csv"
  processed_dir:  "data/processed"
  augmented_dir:  "data/augmented"
  n_features:     64          # After SelectKBest (paper Section 4.1)
  spatial_small:  8           # 8x8 intermediate grid
  spatial_large:  28          # 28x28 bicubic upsampled
  train_ratio:    0.80
  val_ratio:      0.10
  test_ratio:     0.10
  num_classes:    5           # BENIGN + 4 DoS types (Heartbleed excluded)
  class_names:
    - "BENIGN"
    - "DoS_slowloris"
    - "DoS_Slowhttptest"
    - "DoS_Hulk"
    - "DoS_GoldenEye"

# ----------------------------------------------------------
# CTGAN (Phase 2 replacement for SMOTE-ENN)
# Learns joint feature distribution per class via GAN training.
# Produces diverse, realistic synthetic samples vs SMOTE's
# linear interpolation. 15K/class to match Phase 1 total.
# ----------------------------------------------------------
ctgan:
  target_per_class: 15000     # 15K x 5 classes = 75K total (matches Phase 1)
  epochs:           50        # CTGAN training epochs per class
  batch_size:       500       # Must be divisible by pac (default 10)

# ----------------------------------------------------------
# Spatial Feature Extraction
# MobileNetV2 + EfficientNet-B0 ensemble (soft feature averaging)
# Both branches adapted for 28x28 input (stride-1 stem)
# ----------------------------------------------------------
spatial:
  backbone_a:   "mobilenet_v2"      # Local spatial patterns via DSConv
  backbone_b:   "efficientnet_b0"   # Channel-wise attention via SE blocks
  pretrained:   false               # Train from scratch
  in_channels:  1                   # Grayscale
  output_dim:   128                 # Each branch -> 128-d; soft avg -> 128-d
  dropout:      0.5

# ----------------------------------------------------------
# Temporal Feature Extraction -- TranBiLSTM (Paper Tables 7-8)
# Same hyperparameters as Phase 1 for fair comparison.
# SOLE Phase 2 change: softmax attention -> LinearAttentionBlock
# BiLSTM is IDENTICAL to Phase 1 (paper architecture preserved).
# ----------------------------------------------------------
temporal:
  seq_len:        64
  mlp_input_dim:  1
  mlp_hidden_dim: 16    # FC-2 hidden (Table 7)
  d_model:        32    # FC-3 / attention dim (Table 8)
  n_heads:        4     # Attention heads (Table 8)
  ff_hidden:      64    # Feed-forward hidden (Table 8)
  bilstm_hidden:  64    # BiLSTM hidden per direction (Table 8)
  dropout:        0.5

# ----------------------------------------------------------
# Classification Head (Paper Table 9) -- identical to Phase 1
# ----------------------------------------------------------
classification:
  input_dim:   256        # Concat(spatial=128, temporal=128)
  num_classes: 5          # BENIGN + 4 DoS types
  dropout:     0.5

# ----------------------------------------------------------
# Training (Paper Table 10 + Section 4.2)
# All values identical to Phase 1 for fair comparison.
# ----------------------------------------------------------
training:
  batch_size:    128       # Reduced from paper's 256 (FYP runtime constraint)
  learning_rate: 0.0001    # Paper Table 10 (Adam)
  optimizer:     "adam"
  dropout:       0.5       # Paper Table 10
  max_epochs:    30        # Reduced from paper's 100 (FYP runtime constraint)
  patience:      10        # Early stopping
  grad_clip:     1.0
  num_workers:   0         # 0 = no multiprocessing (Windows spawn safety)
  device:        "cuda"

# ----------------------------------------------------------
# Paths (identical structure to Phase 1)
# ----------------------------------------------------------
paths:
  checkpoint_dir:  "results/checkpoints"
  metrics_dir:     "results/metrics"
  plots_dir:       "results/plots"
  log_dir:         "logs"
  tensorboard_dir: "logs/tensorboard"
  